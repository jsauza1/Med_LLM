{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsauza/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "# pip install torch\n",
    "# pip install python-dotenv \n",
    "# ! pip install lxml\n",
    "import os\n",
    "import dotenv\n",
    "import joblib\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch \n",
    "\n",
    "dotenv.load_dotenv()\n",
    "HUGGING_FACE_API_KEY = os.getenv(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Mask Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<generator object _walk at 0x13004d5b0>]\n"
     ]
    }
   ],
   "source": [
    "print([os.walk(root_dir)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def process_xml_files(root_dir):\n",
    "    data = []\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                filepath = subdir + os.sep + file\n",
    "                with open(filepath, 'r', encoding='utf-8') as xml_file:\n",
    "                    soup = BeautifulSoup(xml_file, 'xml')\n",
    "                    qapairs = soup.find_all('QAPair')\n",
    "                    for qapair in qapairs:\n",
    "                        question = qapair.find('Question')\n",
    "                        answer = qapair.find('Answer')\n",
    "                        # Both question and answer exist and are non-empty\n",
    "                        if question and question.text.strip() and answer and answer.text.strip():\n",
    "                            # Clean up the text by replacing tabs, newlines, and multiple spaces with a single space\n",
    "                            clean_question = ' '.join(question.text.split())\n",
    "                            clean_answer = ' '.join(answer.text.split())\n",
    "                            data.append({\n",
    "                                \"instruction\" : \"You are a medical expert and you will answer questions related to medical inquiries.\",\n",
    "                                \"input\": clean_question,\n",
    "                                \"output\": clean_answer,\n",
    "                            })\n",
    "    return data\n",
    "\n",
    "def write_json_file(data, output_file):\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "root_dir = '/Users/jsauza/medical/MedQuad'  # Change this to the root directory of your XML files\n",
    "output_file = 'output.json'  # The file where you want to store your JSON data\n",
    "\n",
    "data = process_xml_files(root_dir)\n",
    "write_json_file(data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 286/286 [00:00<00:00, 685kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 14.5M/14.5M [00:00<00:00, 32.9MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 96.0/96.0 [00:00<00:00, 439kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 784/784 [00:00<00:00, 7.34MB/s]\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 31.9k/31.9k [00:00<00:00, 32.2MB/s]\n",
      "Downloading (…)l-00002-of-00016.bin: 100%|██████████| 2.06G/2.06G [00:46<00:00, 44.1MB/s]\n",
      "Downloading (…)l-00003-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 44.3MB/s]\n",
      "Downloading (…)l-00004-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 44.1MB/s]\n",
      "Downloading (…)l-00005-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 43.1MB/s]\n",
      "Downloading (…)l-00006-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 43.8MB/s]\n",
      "Downloading (…)l-00007-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 44.2MB/s]\n",
      "Downloading (…)l-00008-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 44.0MB/s]\n",
      "Downloading (…)l-00009-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 42.8MB/s]\n",
      "Downloading (…)l-00010-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 43.1MB/s]\n",
      "Downloading (…)l-00011-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 43.6MB/s]\n",
      "Downloading (…)l-00012-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 43.1MB/s]\n",
      "Downloading (…)l-00013-of-00016.bin: 100%|██████████| 940M/940M [00:21<00:00, 43.4MB/s]\n",
      "Downloading (…)l-00014-of-00016.bin: 100%|██████████| 940M/940M [00:22<00:00, 41.7MB/s]\n",
      "Downloading (…)l-00015-of-00016.bin: 100%|██████████| 806M/806M [00:19<00:00, 41.6MB/s]\n",
      "Downloading (…)l-00016-of-00016.bin: 100%|██████████| 2.06G/2.06G [00:49<00:00, 41.2MB/s]\n",
      "Downloading shards: 100%|██████████| 15/15 [06:22<00:00, 25.52s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalGPT-base-zh\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"medicalai/ClinicalGPT-base-zh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"medicalai/ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
    "model_checkpoint = model_id #\"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "pipeline = pipeline(\"fill-mask\", model=model, device=-1, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> ClinicalBERT number of parameters: 135M'\n",
      "'>>> BERT number of parameters: 110M'\n"
     ]
    }
   ],
   "source": [
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> ClinicalBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The patient has a high fever which was indicative of [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> The patient has a high fever which was indicative of infection.'\n",
      "'>>> The patient has a high fever which was indicative of pain.'\n",
      "'>>> The patient has a high fever which was indicative of symptoms.'\n",
      "'>>> The patient has a high fever which was indicative of fever.'\n",
      "'>>> The patient has a high fever which was indicative of influenza.'\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.80s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"google/flan-t5-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "pipeline = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You can build a simple electronic engine by using a battery and a circuit board.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"What do you build a simple electronic engine?\")\n",
    "#pipeline(\"What are competitors to Apache Kafka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
    "\n",
    "model_name = \"WizardLM/WizardCoder-3B-V1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(\"How do you make a lemonade?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
