{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsauza/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ! pip3 install torch torchvision torchaudio\n",
    "# ! pip install transformers\n",
    "# ! pip install python-dotenv \n",
    "# ! pip install auto_gptq==0.2.0\n",
    "# ! pip install optimum\n",
    "import os\n",
    "import dotenv\n",
    "# import joblib\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch \n",
    "\n",
    "dotenv.load_dotenv()\n",
    "HUGGING_FACE_API_KEY = os.getenv(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'macOS-13.3.1-arm64-arm-64bit'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GPU is required to quantize or run quantize model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mTheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mTheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2570\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2566\u001b[0m     quantization_method_from_args \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mGPTQ\n\u001b[1;32m   2567\u001b[0m     \u001b[39mor\u001b[39;00m quantization_method_from_config \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mGPTQ\n\u001b[1;32m   2568\u001b[0m ):\n\u001b[1;32m   2569\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m-> 2570\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGPU is required to quantize or run quantize model.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2571\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_optimum_available() \u001b[39mand\u001b[39;00m is_auto_gptq_available()):\n\u001b[1;32m   2572\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   2573\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLoading GPTQ quantized model requires optimum library : `pip install optimum` and auto-gptq library \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpip install auto-gptq\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2574\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GPU is required to quantize or run quantize model."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
    "\n",
    "# model_name = \"WizardLM/WizardCoder-Python-34B-V1.0\"\n",
    "model_name = \"WizardLM/WizardCoder-3B-V1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(\"How do you make a lemonade?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least one of the model submodule will be offloaded to disk, please pass along an `offload_folder`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, use_fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m model \u001b[39m=\u001b[39m AutoGPTQForCausalLM\u001b[39m.\u001b[39;49mfrom_quantized(model_name_or_path,\n\u001b[1;32m     14\u001b[0m         model_basename\u001b[39m=\u001b[39;49mmodel_basename,\n\u001b[1;32m     15\u001b[0m         use_safetensors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     16\u001b[0m         trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     17\u001b[0m         device_map\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m         use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m     19\u001b[0m         quantize_config\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:82\u001b[0m, in \u001b[0;36mAutoGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, save_dir, device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m quant_func \u001b[39m=\u001b[39m GPTQ_CAUSAL_LM_MODEL_MAP[model_type]\u001b[39m.\u001b[39mfrom_quantized\n\u001b[1;32m     81\u001b[0m keywords \u001b[39m=\u001b[39m {key: kwargs[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m signature(quant_func)\u001b[39m.\u001b[39mparameters \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m kwargs}\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m quant_func(\n\u001b[1;32m     83\u001b[0m     model_name_or_path\u001b[39m=\u001b[39;49mmodel_name_or_path,\n\u001b[1;32m     84\u001b[0m     save_dir\u001b[39m=\u001b[39;49msave_dir,\n\u001b[1;32m     85\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m     86\u001b[0m     max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m     87\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     88\u001b[0m     low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m     89\u001b[0m     use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m     90\u001b[0m     inject_fused_attention\u001b[39m=\u001b[39;49minject_fused_attention,\n\u001b[1;32m     91\u001b[0m     inject_fused_mlp\u001b[39m=\u001b[39;49minject_fused_mlp,\n\u001b[1;32m     92\u001b[0m     use_cuda_fp16\u001b[39m=\u001b[39;49muse_cuda_fp16,\n\u001b[1;32m     93\u001b[0m     quantize_config\u001b[39m=\u001b[39;49mquantize_config,\n\u001b[1;32m     94\u001b[0m     model_basename\u001b[39m=\u001b[39;49mmodel_basename,\n\u001b[1;32m     95\u001b[0m     use_safetensors\u001b[39m=\u001b[39;49muse_safetensors,\n\u001b[1;32m     96\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m     97\u001b[0m     warmup_triton\u001b[39m=\u001b[39;49mwarmup_triton,\n\u001b[1;32m     98\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkeywords\n\u001b[1;32m     99\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:773\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, save_dir, device_map, max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, **kwargs)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m    771\u001b[0m     make_sure_no_tensor_in_meta_device(model, use_triton, quantize_config\u001b[39m.\u001b[39mdesc_act, quantize_config\u001b[39m.\u001b[39mgroup_size)\n\u001b[0;32m--> 773\u001b[0m accelerate\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mmodeling\u001b[39m.\u001b[39;49mload_checkpoint_in_model(\n\u001b[1;32m    774\u001b[0m     model,\n\u001b[1;32m    775\u001b[0m     checkpoint\u001b[39m=\u001b[39;49mmodel_save_name,\n\u001b[1;32m    776\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    777\u001b[0m     offload_state_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    778\u001b[0m     offload_buffers\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    780\u001b[0m model \u001b[39m=\u001b[39m simple_dispatch_model(model, device_map)\n\u001b[1;32m    782\u001b[0m \u001b[39m# == step4: set seqlen == #\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:1271\u001b[0m, in \u001b[0;36mload_checkpoint_in_model\u001b[0;34m(model, checkpoint, device_map, offload_folder, dtype, offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb)\u001b[0m\n\u001b[1;32m   1268\u001b[0m check_tied_parameters_on_same_device(tied_params, device_map)\n\u001b[1;32m   1270\u001b[0m \u001b[39mif\u001b[39;00m offload_folder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m-> 1271\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAt least one of the model submodule will be offloaded to disk, please pass along an `offload_folder`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1273\u001b[0m     )\n\u001b[1;32m   1274\u001b[0m \u001b[39melif\u001b[39;00m offload_folder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m   1275\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(offload_folder, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: At least one of the model submodule will be offloaded to disk, please pass along an `offload_folder`."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import argparse\n",
    "\n",
    "model_name_or_path = \"TheBloke/tulu-13B-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "device = torch.device('mps')\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device_map= \"auto\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "User specified autocast device_type must be 'cuda' or 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m*** Generate:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt_template, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m#.input_ids.cuda()\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49minput_ids, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     30\u001b[0m \u001b[39m# Inference can also be done using transformers' pipeline\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[39m# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:422\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(), torch\u001b[39m.\u001b[39;49mamp\u001b[39m.\u001b[39;49mautocast(device_type\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39;49mtype):\n\u001b[1;32m    423\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:201\u001b[0m, in \u001b[0;36mautocast.__init__\u001b[0;34m(self, device_type, dtype, enabled, cache_enabled)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfast_dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhpu\u001b[39m.\u001b[39mget_autocast_hpu_dtype()  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUser specified autocast device_type must be \u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcuda\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m or \u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m enabled \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mcommon\u001b[39m.\u001b[39mamp_definitely_not_available() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: User specified autocast device_type must be 'cuda' or 'cpu'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.to(device)\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt') #.input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "User specified autocast device_type must be 'cuda' or 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m*** Generate:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt_template, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m#.input_ids.cuda()\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49minput_ids, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     11\u001b[0m \u001b[39m# Inference can also be done using transformers' pipeline\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[39m# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:422\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(), torch\u001b[39m.\u001b[39;49mamp\u001b[39m.\u001b[39;49mautocast(device_type\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39;49mtype):\n\u001b[1;32m    423\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:201\u001b[0m, in \u001b[0;36mautocast.__init__\u001b[0;34m(self, device_type, dtype, enabled, cache_enabled)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfast_dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhpu\u001b[39m.\u001b[39mget_autocast_hpu_dtype()  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUser specified autocast device_type must be \u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcuda\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m or \u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m enabled \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mcommon\u001b[39m.\u001b[39mamp_definitely_not_available() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: User specified autocast device_type must be 'cuda' or 'cpu'"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt') #.input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m prompt_template\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39m### Human: \u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m### Assistant:\u001b[39m\u001b[39m'''\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m*** Generate:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt_template, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49minput_ids\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m     28\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(inputs\u001b[39m=\u001b[39minput_ids, temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[0;32m~/Documents/GitHub/Med_LLM/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import auto_gptq\n",
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import argparse\n",
    "\n",
    "model_name_or_path = \"TheBloke/tulu-13B-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device=torch.device(\"mps\"),\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
